{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import h5py\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "random_state_number = 967898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./Data_2labels_CIB4/df_test.pickle','rb') as fichier:\n",
    "    df_test = pickle.load(fichier)\n",
    "\n",
    "with open ('./Data_2labels_CIB4/df_train.pickle','rb') as fichier:\n",
    "    df_train = pickle.load(fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data frame : ...\n",
      "  Patent_number CIB_1 CIB_2                                               text\n",
      "0     FR1706228  G06F  G06F   CREATION ET GESTION DE TRANSFORMATIONS DE VER...\n",
      "1     FR1706229  G01N  None   dispositif d'etalonnage pour ethylometre L'in...\n",
      "2      FR440980  H05B  None   CIRCUIT POUR EXCITER UNE INSTALLATION DE LAMP...\n",
      "3      FR440981  G06F  H05K   CONNEXIONS POUR DES PLAQUETTES A CIRCUITS IMP...\n",
      "4      FR440982  B05D  B05C   PROCEDE D'APPLICATION D'UNE COUCHE PROTECTRIC...\n",
      "\n",
      "\n",
      "Test data frame : ...\n",
      "  Patent_number CIB_1 CIB_2                                               text\n",
      "0     FR1706257  G01T  A61B   TOMODENSITOMÈTRE À RÉSOLUTION VARIABLE ET À C...\n",
      "1     FR2011487  C22C  C23C   MATÉRIAU COMPOSITE Matériau composite (1) com...\n",
      "2     FR3804361  H04L  H04H   DISTRIBUTION EN INTÉRIEUR D'UN SIGNAL À LARGE...\n",
      "3     FR3804376  G05D  G06Q   SYSTÈME DE DRONE DISTRIBUÉ ET DRONE Pour perm...\n",
      "4      FR508245  H04N  H04N   PROCEDE D'IMAGERIE AMELIORE DESTINE A DES MAT...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data frame : ...\" )\n",
    "print(df_train.head())\n",
    "print('\\n')\n",
    "print(\"Test data frame : ...\" )\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 93103 patents for testing\n",
      "Number of class represented in test Data for CIB_1 only :616\n",
      "\n",
      "There are 4562049 patents for training \n",
      "Number of class represented in training Data for CIB_1 only :633\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} patents for testing\".format(len(df_test)))\n",
    "print(\"Number of class represented in test Data for CIB_1 only :{}\".format(df_test['CIB_1'].nunique()) + '\\n')\n",
    "\n",
    "print(\"There are {} patents for training \".format(len(df_train)))\n",
    "print(\"Number of class represented in training Data for CIB_1 only :{}\".format(df_train['CIB_1'].nunique()) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqAndPad(text,max_length, tokenizer):\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    del text\n",
    "    data = pad_sequences(sequences, maxlen=max_length,\n",
    "                         padding='post', truncating='post')\n",
    "    del sequences\n",
    "    return(data)\n",
    "\n",
    "def truncate(text, max_length):\n",
    "    for o, doc in enumerate(text):\n",
    "        text[o]= \" \".join(text[o].split()[:max_length])\n",
    "    return(text)\n",
    "\n",
    "def convertLabelsDict():\n",
    "    CIBtoLabel={}\n",
    "    id=0\n",
    "    for entry in df_train['CIB_1']:\n",
    "        if entry not in CIBtoLabel:\n",
    "            CIBtoLabel[entry]=id\n",
    "            id+=1\n",
    "    for entry in df_test['CIB_1']:\n",
    "        if entry not in CIBtoLabel:\n",
    "            CIBtoLabel[entry]=id\n",
    "            id+=1\n",
    "    return(CIBtoLabel)\n",
    "\n",
    "def formatTextData (df_train, df_test,max_length, max_num_words):\n",
    "    '''\n",
    "    max_length : tronquer les abstracts\n",
    "    max_num_words : dans le tokenizer, si un mot est trop fréquent, il est supprimé\n",
    "    '''\n",
    "    text_train = df_train['text'].tolist()\n",
    "    text_test = df_test['text'].tolist()\n",
    "    \n",
    "    print(\"Truncating text data to max_length\")\n",
    "    \n",
    "    text_train = truncate(text_train[:200000], max_length)\n",
    "    text_test = truncate(text_test, max_length)\n",
    "    \n",
    "    print(\"Tokenizing data...\")\n",
    "    tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(text_train)\n",
    "    print(\"Tokenizing done\")\n",
    "    print(\"Sequencing and padding...\")\n",
    "    text_train = seqAndPad(text_train, max_length, tokenizer)\n",
    "    text_test = seqAndPad(text_test, max_length, tokenizer)\n",
    "    print(\"Sequencing and padding done\")\n",
    "    \n",
    "    CIBtoLabel = convertLabelsDict()\n",
    "    \n",
    "    y_train = df_train['CIB_1'].tolist()[:200000]\n",
    "    y_test = df_test['CIB_1'].tolist()\n",
    "    \n",
    "    for k in range(len(y_train)):\n",
    "        y_train[k]= CIBtoLabel[y_train[k]]\n",
    "    \n",
    "    for k in range(len(y_test)):\n",
    "        y_test[k]= CIBtoLabel[y_test[k]]\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 633)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 633)\n",
    "    \n",
    "    return(text_train, y_train, text_test, y_test, tokenizer)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating text data to max_length\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, tokenizer = formatTextData(df_train, df_test, max_length = 500, max_num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(embedding_dim, max_length, max_num_words):\n",
    "    texts_to_tokenize = readData('train_texts.pkl', max_length)\n",
    "    print(\"Tokenizing data ..\")\n",
    "    tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(texts_to_tokenize)\n",
    "    print(\"Tokenization done\")\n",
    "    print(\"Loading data and sequencing...\")\n",
    "    x_train, y_train = editData('train_texts.pkl', 'train_labels.pkl',\n",
    "                                max_length, tokenizer)\n",
    "    print(\"Training data loaded\") ###Maintenant on fait le sequencing et le padding, qui sont dans la fonction editdata\n",
    "    x_val, y_val = editData('test_texts.pkl', 'test_labels.pkl', \n",
    "                            max_length, tokenizer)\n",
    "    print(\"Test Data loaded\")\n",
    "    word_index = tokenizer.word_index\n",
    "    return x_train, y_train, x_val, y_val, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMB_SIZE = 200\n",
    "MAX_TEXT_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000\n",
      "(90000, 614)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9989\n",
      "(9989, 614)\n"
     ]
    }
   ],
   "source": [
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "\n",
    "## Prepairing the Embedding layer\n",
    "\n",
    "We compute an index mapping words to known pre-trained embeddings, by parsing the data dump of pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size is 92098\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index) + 1\n",
    "print('Vocab_size is {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('./Embeddings/glove.6B.200d.txt', 'r', encoding= \"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word=values[0]\n",
    "        emb = np.asarray( values[1:], dtype='float32')\n",
    "        embeddings_index[word] = emb\n",
    "print('Found {} word vectors'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_matrix loaded\n",
      "Shape (92098, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, WORD_EMB_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Embedding_matrix loaded')\n",
    "print('Shape {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.engine import Layer, InputSpec, InputLayer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.layers import Dropout, Embedding, concatenate\n",
    "from keras.layers import Conv1D, MaxPool1D, Conv2D, MaxPool2D, ZeroPadding1D\n",
    "from keras.layers import Dense, Input, Flatten, BatchNormalization\n",
    "from keras.layers import Concatenate, Dot, Concatenate, Multiply, RepeatVector\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Lambda, Permute\n",
    "\n",
    "from keras.layers.core import Reshape, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Max Pooling Layer\n",
    "\n",
    "<a href=https://github.com/bicepjai/Deep-Survey-Text-Classification/blob/master/deep_models/paper_02_cnn_sent_model/utils.py> Lien ici </a>\n",
    "\n",
    "et <a href=https://www.reddit.com/r/learnmachinelearning/comments/9hes2q/code_question_1d_convolution_layer_in_keras_with/> la </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import KMaxPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "CNN with Dynamic k-Max Pooling with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters \"\"\"\n",
    "FILTERS = 128\n",
    "pooling_units = 100\n",
    "output_dims = 633\n",
    "\n",
    "text_seq_input = Input(shape=(MAX_TEXT_LEN,), dtype='int32')\n",
    "text_embedding = Embedding(vocab_size, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "                            weights=[embedding_matrix], trainable=True)(text_seq_input)\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "convs = []\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=FILTERS, kernel_size=filter_size, padding='same', activation='relu')(text_embedding)\n",
    "    POOL_SIZE = l_conv.get_shape()[-2] // pooling_units\n",
    "    l_pool = MaxPool1D(pool_size=POOL_SIZE, padding='valid')(l_conv)   #Dynamic pooling\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "# since the text is too long we are maxpooling over 100\n",
    "# and not GlobalMaxPool1D\n",
    "l_pool1 = MaxPool1D(100)(l_cov1)\n",
    "l_flat = Flatten()(l_pool1)\n",
    "l_hidden = Dense(512, activation='relu')(l_flat)\n",
    "l_hidden_drop = Dropout(0.5)(l_hidden)\n",
    "l_out = Dense(output_dims, activation='sigmoid')(l_hidden_drop)  #dims output\n",
    "model_1 = Model(inputs=[text_seq_input], outputs=l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1000, 200)    18419600    input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 1000, 128)    76928       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 1000, 128)    102528      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1000, 128)    128128      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 100, 128)     0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 100, 128)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 100, 128)     0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 300, 128)     0           max_pooling1d_24[0][0]           \n",
      "                                                                 max_pooling1d_25[0][0]           \n",
      "                                                                 max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 296, 128)     82048       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 2, 128)       0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 256)          0           max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          131584      flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1200)         615600      dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 19,556,416\n",
      "Trainable params: 19,556,416\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "categorical cross-entropy : activation function en entrée c' est un softmax, i.e les scores se somment à 1 \n",
    "alors que binary cross-entropy, c'est du softmax.\n",
    "'''\n",
    "model_1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This callback logs events for TensorBoard, including:\n",
    "log_dir : directory to save log file to be parsed by TensorBoard\n",
    "histogram_freq : frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed\n",
    "write_graph : whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.\n",
    "write_images : whether to write model weights to visualize as image in TensorBoard.\n",
    "'''\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./tb_graphs', histogram_freq=0, write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"model_1_weights.hdf5\", \n",
    "                                    verbose=1,\n",
    "                                    monitor=\"val_categorical_accuracy\",\n",
    "                                    save_best_only=True,\n",
    "                                    mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Monitor : Quantity to be monitored\n",
    "    min_delta : Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience : Number of epochs with no improvement after which training will be stopped\n",
    "    mode :One of {\"auto\", \"min\", \"max\"}\n",
    "    '''\n",
    "earlystopping = EarlyStopping(monitor='val_categorical_accuracy', \n",
    "                              min_delta=0, patience=5, \n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoints available !\n",
      "Train on 90000 samples, validate on 9989 samples\n",
      "Epoch 1/10\n",
      "  800/90000 [..............................] - ETA: 39:46 - loss: 6.0502 - categorical_accuracy: 0.0613"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-f90e7e85fef1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m           callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    model_1.load_weights(\"model_11_weights.hdf5\")\n",
    "except IOError as ioe:\n",
    "    print(\"no checkpoints available !\")\n",
    "    \n",
    "model_1.fit(x_train, y_train, \n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=10, batch_size=32,shuffle=True,\n",
    "          callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
