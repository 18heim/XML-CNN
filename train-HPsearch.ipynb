{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import h5py\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "random_state_number = 967898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "##Check if it runs on GPU\n",
    "\n",
    "# Create some tensors\n",
    "# Place tensors on default config (hopefully GPU)\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./Data_2labels_CIB4/df_test_clean.pickle','rb') as fichier:\n",
    "    df_test = pickle.load(fichier)\n",
    "\n",
    "with open ('./Data_2labels_CIB4/df_train_clean.pickle','rb') as fichier:\n",
    "    df_train = pickle.load(fichier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data frame : ...\n",
      "2    circuit exciter installation lampes a decharge...\n",
      "4    procede d'application d'une couche protectrice...\n",
      "5    prise controle circuits integres prise contrôl...\n",
      "6    dispositif formant ecran electromagnetique dis...\n",
      "7    ballast electronique lampe luminescente a gaz ...\n",
      "Name: text, dtype: object\n",
      "\n",
      "\n",
      "Test data frame : ...\n",
      "0    tomodensitomètre résolution variable capacité ...\n",
      "1    matériau composite matériau composite (1) comp...\n",
      "2    distribution intérieur d'un signal large bande...\n",
      "3    système drone distribué drone permettre d'obte...\n",
      "4    procede d'imagerie ameliore destine a matieres...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data frame : ...\" )\n",
    "print(df_train['text'].head())\n",
    "print('\\n')\n",
    "print(\"Test data frame : ...\" )\n",
    "print(df_test['text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 91519 patents for testing\n",
      "Number of class represented in test Data for CIB_1 only :435\n",
      "\n",
      "There are 3751492 patents for training \n",
      "Number of class represented in training Data for CIB_1 only :425\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} patents for testing\".format(len(df_test)))\n",
    "print(\"Number of class represented in test Data for CIB_1 only :{}\".format(df_test['CIB_1'][:100000].nunique()) + '\\n')\n",
    "\n",
    "print(\"There are {} patents for training \".format(len(df_train)))\n",
    "print(\"Number of class represented in training Data for CIB_1 only :{}\".format(df_train['CIB_1'][:1000000].nunique()) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqAndPad(text,max_length, tokenizer):\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    del text\n",
    "    data = pad_sequences(sequences, maxlen=max_length,\n",
    "                         padding='post', truncating='post')\n",
    "    del sequences\n",
    "    return(data)\n",
    "\n",
    "def truncate(text, max_length):\n",
    "    for o, doc in enumerate(text):\n",
    "        text[o]= \" \".join(text[o].split()[:max_length])\n",
    "    return(text)\n",
    "\n",
    "def convertLabelsDict():\n",
    "    CIBtoLabel={}\n",
    "    id=0\n",
    "    for entry in df_train['CIB_1']:\n",
    "        if entry not in CIBtoLabel:\n",
    "            CIBtoLabel[entry]=id\n",
    "            id+=1\n",
    "    for entry in df_test['CIB_1']:\n",
    "        if entry not in CIBtoLabel:\n",
    "            CIBtoLabel[entry]=id\n",
    "            id+=1\n",
    "    return(CIBtoLabel)\n",
    "\n",
    "def formatTextData (df_train, df_test,max_length, max_num_words):\n",
    "    '''\n",
    "    max_length : tronquer les abstracts\n",
    "    max_num_words : dans le tokenizer, si un mot est trop fréquent, il est supprimé\n",
    "    '''\n",
    "    text_train = df_train['text'].tolist()\n",
    "    text_test = df_test['text'].tolist()\n",
    "    \n",
    "    print(\"Truncating text data to max_length\")\n",
    "    \n",
    "    text_train = truncate(text_train[:1500000], max_length)\n",
    "    text_test = truncate(text_test, max_length)\n",
    "    \n",
    "    print(\"Tokenizing data...\")\n",
    "    tokenizer = Tokenizer(num_words=max_num_words)\n",
    "    tokenizer.fit_on_texts(text_train)\n",
    "    print(\"Tokenizing done\")\n",
    "    print(\"Sequencing and padding...\")\n",
    "    text_train = seqAndPad(text_train, max_length, tokenizer)\n",
    "    text_test = seqAndPad(text_test, max_length, tokenizer)\n",
    "    print(\"Sequencing and padding done\")\n",
    "    \n",
    "    CIBtoLabel = convertLabelsDict()\n",
    "    \n",
    "    y_train = df_train['CIB_1'].tolist()[:1500000]\n",
    "    y_test = df_test['CIB_1'].tolist()\n",
    "    \n",
    "    for k in range(len(y_train)):\n",
    "        y_train[k]= CIBtoLabel[y_train[k]]\n",
    "    \n",
    "    for k in range(len(y_test)):\n",
    "        y_test[k]= CIBtoLabel[y_test[k]]\n",
    "    \n",
    "    y_train = tf.keras.utils.to_categorical(y_train, 435)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, 435)\n",
    "    \n",
    "    return(text_train, y_train, text_test, y_test, tokenizer)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating text data to max_length\n",
      "Tokenizing data...\n",
      "Tokenizing done\n",
      "Sequencing and padding...\n",
      "Sequencing and padding done\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test, tokenizer = formatTextData(df_train,\n",
    "                                                             df_test, \n",
    "                                                             max_length = 1000,\n",
    "                                                             max_num_words = 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMB_SIZE = 300\n",
    "MAX_TEXT_LEN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 435)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91519, 435)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.externals import joblib\n",
    "# with open (os.path.join('./Data_2labels_CIB4/', 'x_train_y.sav') , 'wb') as save:\n",
    "#    joblib.dump( (x_train,y_train) , save)\n",
    "# with open (os.path.join('./Data_2labels_CIB4/', 'x_test_y.sav') , 'wb') as save:\n",
    "#    joblib.dump( (x_test,y_test) , save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "\n",
    "## Prepairing the Embedding layer\n",
    "\n",
    "We compute an index mapping words to known pre-trained embeddings, by parsing the data dump of pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab_size is 403676\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index) + 1\n",
    "print('Vocab_size is {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import codecs\n",
    "embeddings_index = {}\n",
    "with codecs.getreader(\"utf-8\")(gzip.open('./Embeddings/cc.fr.300.vec.gz', 'rb')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        emb = np.asarray( values[1:], dtype='float32')\n",
    "        embeddings_index[word] = emb\n",
    "print('Found {} word vectors'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding_matrix loaded\n",
      "Shape (403676, 300)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, WORD_EMB_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding index will be all-zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Embedding_matrix loaded')\n",
    "print('Shape {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape (318134, 200)\n"
     ]
    }
   ],
   "source": [
    "#Case of OOM\n",
    "embedding_matrix = embedding_matrix[:,:200]\n",
    "print('Shape {}'.format(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.engine import Layer, InputSpec, InputLayer\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.layers import Dropout, Embedding, concatenate\n",
    "from keras.layers import Conv1D, MaxPool1D, Conv2D, MaxPool2D, ZeroPadding1D\n",
    "from keras.layers import Dense, Input, Flatten, BatchNormalization\n",
    "from keras.layers import Concatenate, Dot, Concatenate, Multiply, RepeatVector\n",
    "from keras.layers import Bidirectional, TimeDistributed\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Lambda, Permute\n",
    "\n",
    "from keras.layers.core import Reshape, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#from tensorflow.keras.engine import Layer, InputSpec, InputLayer\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, concatenate\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, Conv2D, MaxPool2D, ZeroPadding1D\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Dot, Concatenate, Multiply, RepeatVector\n",
    "from tensorflow.keras.layers import Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Lambda, Permute\n",
    "\n",
    "#from tensorflow.keras.layers.core import Reshape, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "#from tensorflow.keras.constraints import maxnorm\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Max Pooling Layer\n",
    "\n",
    "<a href=https://github.com/bicepjai/Deep-Survey-Text-Classification/blob/master/deep_models/paper_02_cnn_sent_model/utils.py> Lien ici </a>\n",
    "\n",
    "et <a href=https://www.reddit.com/r/learnmachinelearning/comments/9hes2q/code_question_1d_convolution_layer_in_keras_with/> la </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import KMaxPooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folding layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "CNN with Dynamic k-Max Pooling with sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Grid Search Parameters'''\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "HP_FILTERS = hp.HParam('number_of_filter_channel', hp.Discrete([64,128,256]))\n",
    "HP_POOLING_UNITS = hp.HParam('pooling_units', hp.Discrete([100,500,1000]))\n",
    "HP_HIDDEN_DIMS = hp.HParam('hidden_dims', hp.Discrete([32,64,128]))\n",
    "#HP_EMBEDDING_TRAINABLE = hp.HParam('embedding_trainable' , hp.Discrete(['True', 'False'])) # On ne va pas s'en occuper dans un premier temps\n",
    "                                                                                                # Selon la publication, trainable = False\n",
    "METRIC_CAT_ACCURACY = 'categorical_accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_FILTERS, HP_POOLING_UNITS, HP_HIDDEN_DIMS],\n",
    "    metrics=[hp.Metric(METRIC_CAT_ACCURACY, display_name='Categorical_accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parameters \n",
    "FILTERS = 128\n",
    "pooling_units = 100  #le nombre de features qu'on garde par pooling, i.e après pooling, chaque vecteur est de longueur 100\n",
    "hidden_dims= 32\"\"\"\n",
    "\n",
    "def create_model(hparams, output_dims = 435, Embedding_trainable = False, MAX_TEXT_LEN = 1000):\n",
    "\n",
    "    text_seq_input = Input(shape=(MAX_TEXT_LEN,), dtype='int32')\n",
    "    text_embedding = Embedding(vocab_size, WORD_EMB_SIZE, input_length=MAX_TEXT_LEN,\n",
    "                                weights=[embedding_matrix], trainable=Embedding_trainable)(text_seq_input)\n",
    "    text_dropout = Dropout(0.25)(text_embedding)\n",
    "\n",
    "    filter_sizes = [2,3,4,8,10]\n",
    "    convs = []\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=hparams[HP_FILTERS], kernel_size=filter_size, padding='same', activation='relu')(text_dropout)\n",
    "        POOL_SIZE = l_conv.get_shape()[-2] // hparams[HP_POOLING_UNITS]\n",
    "        l_pool = MaxPool1D(pool_size=POOL_SIZE, strides =2, padding='valid')(l_conv)   #Dynamic pooling\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = Concatenate(axis=1)(convs)\n",
    "    l_cov1= Conv1D(128, 5, activation='relu')(l_merge)\n",
    "    # since the text is too long we are maxooling over 100\n",
    "    # and not GlobalMaxPool1D\n",
    "    l_pool1 = MaxPool1D(100)(l_cov1)\n",
    "    l_flat = Flatten()(l_pool1)\n",
    "    #l_flat = Flatten()(l_merge)\n",
    "    l_hidden = Dense(hparams[HP_HIDDEN_DIMS], activation ='relu')(l_flat)\n",
    "    l_hidden_drop = Dropout(0.5)(l_hidden)\n",
    "    l_out = Dense(output_dims, activation='softmax')(l_hidden_drop)  #dims output\n",
    "    model_1 = Model(inputs=[text_seq_input], outputs=l_out)\n",
    "    \n",
    "    return(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e9dff87424bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0malors\u001b[0m \u001b[0mque\u001b[0m \u001b[0mbinary\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mest\u001b[0m \u001b[0mdu\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m '''\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "''' \n",
    "categorical cross-entropy : activation function en entrée c' est un softmax, i.e les scores se somment à 1 \n",
    "alors que binary cross-entropy, c'est du sigmoid.\n",
    "'''\n",
    "model_1.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This callback logs events for TensorBoard, including:\n",
    "log_dir : directory to save log file to be parsed by TensorBoard\n",
    "histogram_freq : frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed\n",
    "write_graph : whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.\n",
    "write_images : whether to write model weights to visualize as image in TensorBoard.\n",
    "'''\n",
    "\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True, profile_batch = 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c6d1d4bbb4f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhp_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasCallback\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./logs/hparam_tuning'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hparams' is not defined"
     ]
    }
   ],
   "source": [
    "hp_callback = hp.KerasCallback( hparams, log_dir = './logs/hparam_tuning' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"model_1_weights.hdf5\", \n",
    "                                    verbose=1,\n",
    "                                    monitor=\"val_categorical_accuracy\",\n",
    "                                    save_best_only=True,\n",
    "                                    mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Monitor : Quantity to be monitored\n",
    "    min_delta : Minimum change in the monitored quantity to qualify as an improvement\n",
    "    patience : Number of epochs with no improvement after which training will be stopped\n",
    "    mode :One of {\"auto\", \"min\", \"max\"}\n",
    "    '''\n",
    "earlystopping = EarlyStopping(monitor='val_categorical_accuracy', \n",
    "                              min_delta=0, patience=5, \n",
    "                              verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(x_train, y_train, x_test, y_test):\n",
    "    batch_size = 96\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset = get_dataset(x_train, y_train, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    model_1 = create_model(hparams, output_dims = 435, Embedding_trainable = False, MAX_TEXT_LEN = 1000)\n",
    "    model_1.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate= 0.015), metrics=['categorical_accuracy'])\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs',\n",
    "                                                 histogram_freq=0, \n",
    "                                                 write_graph=True, \n",
    "                                                 profile_batch = 100000000)\n",
    "    hp_callback = hp.KerasCallback(run_dir,  hparams )\n",
    "    tbde_callback = tf.keras.callbacks.TensorBoard(run_dir)\n",
    "    model_1.fit(train_dataset,\n",
    "                validation_data = dev_dataset, \n",
    "                epochs=10, batch_size=96,shuffle=True,\n",
    "                callbacks=[tb_callback, hp_callback, tbde_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d6b973a57719>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.tensorboard'"
     ]
    }
   ],
   "source": [
    "from tensorflow.tensorboard.tensorboard import main\n",
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2')\n",
      "Number of devices: 3\n",
      "--- Starting trial: run-0\n",
      "{'number_of_filter_channel': 64, 'pooling_units': 100, 'hidden_dims': 32}\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 16 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "  234/15625 [..............................] - ETA: 10:20 - loss: 5.7415 - categorical_accuracy: 0.0167"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    session_num = 0\n",
    "    for filters in HP_FILTERS.domain.values:\n",
    "        for pooling_units in HP_POOLING_UNITS.domain.values:\n",
    "            for hidden_dims in HP_HIDDEN_DIMS.domain.values:\n",
    "                hparams = {\n",
    "                    HP_FILTERS : filters,\n",
    "                    HP_POOLING_UNITS : pooling_units,\n",
    "                    HP_HIDDEN_DIMS : hidden_dims,\n",
    "                }\n",
    "                run_name = \"run-%d\" % session_num\n",
    "                print('--- Starting trial: %s' % run_name)\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                run('logs/hparam_tuning/' + run_name, hparams)\n",
    "                session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoints available !\n",
      "Epoch 1/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0141 - categorical_accuracy: 0.0428\n",
      "Epoch 00001: val_categorical_accuracy improved from 0.06942 to 0.08681, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1034s 662ms/step - loss: 0.0141 - categorical_accuracy: 0.0428 - val_loss: 0.0196 - val_categorical_accuracy: 0.0868\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0128 - categorical_accuracy: 0.0822\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.08681 to 0.13789, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1026s 657ms/step - loss: 0.0128 - categorical_accuracy: 0.0822 - val_loss: 0.0190 - val_categorical_accuracy: 0.1379\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0119 - categorical_accuracy: 0.1217\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.13789 to 0.17176, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1024s 655ms/step - loss: 0.0119 - categorical_accuracy: 0.1217 - val_loss: 0.0187 - val_categorical_accuracy: 0.1718\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0111 - categorical_accuracy: 0.1593\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.17176 to 0.20477, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1024s 655ms/step - loss: 0.0111 - categorical_accuracy: 0.1593 - val_loss: 0.0183 - val_categorical_accuracy: 0.2048\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0103 - categorical_accuracy: 0.2105\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.20477 to 0.22909, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1025s 656ms/step - loss: 0.0103 - categorical_accuracy: 0.2105 - val_loss: 0.0181 - val_categorical_accuracy: 0.2291\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0095 - categorical_accuracy: 0.2635\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.22909 to 0.24758, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1027s 657ms/step - loss: 0.0095 - categorical_accuracy: 0.2635 - val_loss: 0.0179 - val_categorical_accuracy: 0.2476\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0090 - categorical_accuracy: 0.2905\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.24758 to 0.25365, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1026s 656ms/step - loss: 0.0090 - categorical_accuracy: 0.2905 - val_loss: 0.0180 - val_categorical_accuracy: 0.2537\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0087 - categorical_accuracy: 0.3100\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.25365 to 0.26015, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1026s 656ms/step - loss: 0.0087 - categorical_accuracy: 0.3100 - val_loss: 0.0182 - val_categorical_accuracy: 0.2602\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0083 - categorical_accuracy: 0.3346\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.26015 to 0.26651, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1030s 659ms/step - loss: 0.0083 - categorical_accuracy: 0.3346 - val_loss: 0.0183 - val_categorical_accuracy: 0.2665\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - ETA: 0s - loss: 0.0080 - categorical_accuracy: 0.3596\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.26651 to 0.27143, saving model to model_1_weights.hdf5\n",
      "1563/1563 [==============================] - 1022s 654ms/step - loss: 0.0080 - categorical_accuracy: 0.3596 - val_loss: 0.0183 - val_categorical_accuracy: 0.2714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd2b05298d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    model_1.load_weights(\"model_11_weights.hdf5\")\n",
    "except IOError as ioe:\n",
    "    print(\"no checkpoints available !\")\n",
    "    \n",
    "model_1.fit(x_train, y_train, \n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=10, batch_size=128,shuffle=True,\n",
    "          callbacks=[checkpointer, tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('model_1_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2985    9    0 ...    0    0    0]\n",
      " [1222  747    0 ...    0    0    0]\n",
      " [ 224    0  153 ...    0    0    0]\n",
      " ...\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]\n",
      " [   1    0    0 ...    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = model.predict(x_test)\n",
    "y_pred = (predictions > 0.5)\n",
    "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confusionmatrix import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cib_list = convertLabelsDict()\n",
    "cib_list = list(cib_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-3f36b536e310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcib_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/XML-CNN/confusionmatrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(x = x_test, y = y_test, model = model_1)\n",
    "cm.plot_confusion_matrix(cib_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
